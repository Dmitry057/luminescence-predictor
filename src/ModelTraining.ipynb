{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM,AutoModelForSequenceClassification, AutoTokenizer\n",
    "import re\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "from rdkit import RDLogger \n",
    "import pandas as pd\n",
    "from transformers import T5ForSequenceClassification, T5Config\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "atoms_tokens = ['Ag','Al','As','Au','B','Ba','Bi','Br','C','Ca',\n",
    "              'Cd','Cl','Co','Cr','Cs','Cu','F','Fe','Ga','Gd',\n",
    "              'Ge','H','Hg','I','In','K','Li','M','Mg','Mn',\n",
    "              'Mo','N','Na','O','P','Pt','Ru','S','Sb','Sc',\n",
    "              'Se','Si','Sn','V','W','Z','Zn','c','e','n','o','p','s']\n",
    "atoms_tokens = sorted(atoms_tokens, key=lambda s: len(s), reverse=True)\n",
    "SMI_REGEX_PATTERN = r\"(\\[|\\]|\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9]|\" + \\\n",
    "                                                                  '|'.join(atoms_tokens) + \")\"\n",
    "regex = re.compile(SMI_REGEX_PATTERN)\n",
    "def clean_output_sequence(output_sequence):\n",
    "    return output_sequence.replace('</s>', '').replace('<sm_', '').replace(' sm_', '').replace('>', '').strip()\n",
    "def add_special_symbols(text):\n",
    "  output = []\n",
    "  for word in text.split():\n",
    "      tokens = [token for token in regex.findall(word)]\n",
    "      if len(tokens) > 4 and (word == ''.join(tokens)) and MolFromSmiles(word):\n",
    "          output.append(''.join(['<sm_'+t+'>' for t in tokens]))\n",
    "      else:\n",
    "          output.append(word)\n",
    "  return ' '.join(output)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_data = pd.DataFrame(pd.read_csv('../train_split_fluor.csv'))[['Chromophore','Solvent','Absorption max (nm)','Emission max (nm)','Quantum yield']].dropna()\n",
    "validate_data  = pd.DataFrame(pd.read_csv('../test_split_fluor.csv'))[['Chromophore','Solvent','Absorption max (nm)','Emission max (nm)','Quantum yield']].dropna()\n",
    "\n",
    "df = pd.DataFrame(train_data).dropna()\n",
    "df[\"input_text\"] = df[\"Chromophore\"] + \" \" + df[\"Solvent\"]\n",
    "\n",
    "# Separating features and targets\n",
    "X = df[\"input_text\"].tolist()\n",
    "#, \"Emission max (nm)\", \"Quantum yield\"\n",
    "y = df[[\"Absorption max (nm)\"]].values.tolist()\n",
    "\n",
    "# Random shuffle the data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Split the indices for training and evaluation\n",
    "split = int(len(indices) * 0.8)  # 60% train, 20% eval\n",
    "train_indices = indices[:split]\n",
    "eval_indices = indices[split:]\n",
    "\n",
    "# Create train and eval datasets\n",
    "X_train = [add_special_symbols(X[i]) for i in train_indices]\n",
    "y_train = [y[i] for i in train_indices]  # Use train_indices directly\n",
    "X_eval = [add_special_symbols(X[i]) for i in eval_indices]\n",
    "y_eval = [y[i] for i in eval_indices]  # Use eval_indices directly\n",
    "\n",
    "train_labels = torch.tensor(y_train).float()\n",
    "eval_labels = torch.tensor(y_eval).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = CustomDataset(tokenizer(X_train), train_labels)\n",
    "# eval_dataset = CustomDataset(tokenizer(X_eval), eval_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = T5Config.from_pretrained('output/checkpoint-8684')\n",
    "config.num_labels=1\n",
    "model = T5ForSequenceClassification.from_pretrained('output/checkpoint-8684',\n",
    " config=config,\n",
    " ignore_mismatched_sizes=True, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('output/checkpoint-8684')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8684 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the following Chromophore and Solvent, please provide an Absorption max (nm): <sm_C><sm_N><sm_(><sm_C><sm_)><sm_c><sm_1><sm_c><sm_c><sm_c><sm_2><sm_n><sm_c><sm_3><sm_c><sm_c><sm_c><sm_(><sm_N><sm_(><sm_C><sm_)><sm_C><sm_)><sm_c><sm_c><sm_3><sm_[><sm_s><sm_+><sm_]><sm_c><sm_2><sm_c><sm_1> CCO\n",
      "Generated 1 predictions\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_sample(sample):\n",
    "    # Prepare the input string\n",
    "\n",
    "    prompt = add_special_symbols(\"Given the following Chromophore and Solvent, please provide an Absorption max (nm): \" + str(sample))\n",
    "    print(prompt)\n",
    "    # Tokenize the input\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        prompt,\n",
    "        max_length=100,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move the input to the appropriate device\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "    \n",
    "    # Generate the prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    return tokenizer.decode(outputs.logits.argmax(dim=-1)[0], skip_special_tokens=True)\n",
    "   \n",
    "\n",
    "# Process all samples in X_train\n",
    "predictions = []\n",
    "i = 1\n",
    "for sample in tqdm(X_train):\n",
    "    prediction = process_sample(sample)\n",
    "    predictions.append(prediction)\n",
    "    i -= 1\n",
    "    if (i == 0):\n",
    "        break\n",
    "\n",
    "print(f\"Generated {len(predictions)} predictions\")\n",
    "for i in predictions:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tetkin/.venv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2171' max='2171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2171/2171 05:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>99689.480000</td>\n",
       "      <td>99519.265625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['transformer.encoder.embed_tokens.weight', 'transformer.decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./output/nach-model-1/tokenizer_config.json',\n",
       " './output/nach-model-1/special_tokens_map.json',\n",
       " './output/nach-model-1/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    \n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    # run_name=run_name,\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    # per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    resume_from_checkpoint=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset =eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "model.save_pretrained('./output/nach-model-1')\n",
    "tokenizer.save_pretrained('./output/nach-model-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "476018      C   /home/tetkin/.venv/bin/python                7638MiB |\n",
    "|    0   N/A  N/A    kill 498203      C   /home/kruchkov/biot5/.venv/bin/python3      18676MiB |\n",
    "|    1   N/A  N/A    kill 476018      C   /home/tetkin/.venv/bin/python               10762MiB |\n",
    "|    2   N/A  N/A    kill 476018      C   /home/tetkin/.venv/bin/python               10866MiB |\n",
    "|    3   N/A  N/A    kill 114147      C   ...iniconda3/envs/torch_311/bin/python        446MiB |\n",
    "|    3   N/A  N/A    kill 124414      C   ...iniconda3/envs/torch_311/bin/python      16874MiB |\n",
    "|    3   N/A  N/A    kill 387586      C   ...iniconda3/envs/torch_311/bin/python      13900MiB |\n",
    "|    3   N/A  N/A    kill 409845      C   ...iniconda3/envs/torch_311/bin/python        396MiB |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./output/nach-model-1/tokenizer_config.json',\n",
       " './output/nach-model-1/special_tokens_map.json',\n",
       " './output/nach-model-1/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
