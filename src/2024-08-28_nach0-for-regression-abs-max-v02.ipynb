{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zinoviev/miniconda3/envs/airi-summer-p16/bin/python\n",
      "/home/zinoviev/2024-08-23_Project-16\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import re\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "import string\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "atoms_tokens = ['Ag','Al','As','Au','B','Ba','Bi','Br','C','Ca',\n",
    "              'Cd','Cl','Co','Cr','Cs','Cu','F','Fe','Ga','Gd',\n",
    "              'Ge','H','Hg','I','In','K','Li','M','Mg','Mn',\n",
    "              'Mo','N','Na','O','P','Pt','Ru','S','Sb','Sc',\n",
    "              'Se','Si','Sn','V','W','Z','Zn','c','e','n','o','p','s']\n",
    "atoms_tokens = sorted(atoms_tokens, key=lambda s: len(s), reverse=True)\n",
    "SMI_REGEX_PATTERN = r\"(\\[|\\]|\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9]|\" + \\\n",
    "                                                                  '|'.join(atoms_tokens) + \")\"\n",
    "regex = re.compile(SMI_REGEX_PATTERN)\n",
    "def clean_output_sequence(output_sequence):\n",
    "    return output_sequence.replace('</s>', '').replace('<sm_', '').replace(' sm_', '').replace('>', '').strip()\n",
    "\n",
    "def add_special_symbols(text):\n",
    "  output = []\n",
    "  for word in text.split():\n",
    "      tokens = [token for token in regex.findall(word)]\n",
    "      if len(tokens) > 4 and (word == ''.join(tokens)) and MolFromSmiles(word):\n",
    "          output.append(''.join(['<sm_'+t+'>' for t in tokens]))\n",
    "      else:\n",
    "          output.append(word)\n",
    "  return ' '.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas \n",
    "\n",
    "df_train = pandas.read_csv(\"_data/train_split_fluor.csv\")\n",
    "df_train[~df_train[\"Absorption max (nm)\"].isna()][[\"Chromophore\", \"Solvent\", \"Absorption max (nm)\"]].to_csv(\"train_absorption.csv\", index=False)\n",
    "\n",
    "df_test = pandas.read_csv(\"_data/test_split_fluor.csv\")\n",
    "df_test[~df_test[\"Absorption max (nm)\"].isna()][[\"Chromophore\", \"Solvent\", \"Absorption max (nm)\"]].to_csv(\"test_absorption.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e41fc2bebbb544e8a69a4fbaaa0e5106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e905586f4e2459bbbb7f3ebe3b7ec6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\":\"train_absorption.csv\", \"test\":\"test_absorption.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at insilicomedicine/nach0_base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForSequenceClassification, T5Config\n",
    "\n",
    "config = T5Config.from_pretrained('insilicomedicine/nach0_base')\n",
    "config.num_labels=1\n",
    "model = T5ForSequenceClassification.from_pretrained('insilicomedicine/nach0_base',\n",
    " config=config,\n",
    " ignore_mismatched_sizes=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('insilicomedicine/nach0_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess at 0x7fc62801d580> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3fac49a40c45508e754d362cf20b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15649 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1415e50f1ba2456c83fe271b8c4d3ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1646 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(d):\n",
    "    prompt = add_special_symbols(f\"{d[\"Chromophore\"]}, {d[\"Solvent\"]}\")\n",
    "    inputs = tokenizer(prompt,padding=\"longest\", max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "    inputs[\"input_ids\"] = inputs[\"input_ids\"][0]\n",
    "    inputs[\"attention_mask\"] = inputs[\"attention_mask\"][0]\n",
    "    inputs[\"label\"] = d[\"Absorption max (nm)\"]\n",
    "    return inputs\n",
    "\n",
    "dataset_map = dataset.map(preprocess, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19570' max='19570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19570/19570 57:40, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>108442.384000</td>\n",
       "      <td>73679.757812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>37259.856000</td>\n",
       "      <td>22399.857422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>12803.386000</td>\n",
       "      <td>10412.444336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8590.442000</td>\n",
       "      <td>6585.351074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6028.558500</td>\n",
       "      <td>4738.245605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4590.388500</td>\n",
       "      <td>3956.916260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3766.239500</td>\n",
       "      <td>2971.237549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3043.654000</td>\n",
       "      <td>2604.481201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2925.155000</td>\n",
       "      <td>2507.635742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2894.338000</td>\n",
       "      <td>2483.632812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['transformer.encoder.embed_tokens.weight', 'transformer.decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=19570, training_loss=21892.981677950946, metrics={'train_runtime': 3461.1938, 'train_samples_per_second': 45.213, 'train_steps_per_second': 5.654, 'total_flos': 2.267919237221946e+16, 'train_loss': 21892.981677950946, 'epoch': 10.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification, DataCollatorForLanguageModeling, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "run_name = \"2024-08-28_run-005_model-nach0_pred-abs-max_learning-rate-1e-4_epochs-10_batch-size-08_NB-V02\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=run_name,\n",
    "    # run_name=run_name,\n",
    "    learning_rate=5e-5,\n",
    "    # learning_rate=3e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    # per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    resume_from_checkpoint=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_map[\"train\"],\n",
    "    eval_dataset=dataset_map[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
