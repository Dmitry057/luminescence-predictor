{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f56f244-f7cd-4379-a079-a98423f5c0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1,2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc853d39-945d-4629-865f-72edb83fbc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d173bae5-bf55-4131-83dc-573580addc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f2183cd-945c-49e7-a8d8-478175c709ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5ForRegression(T5ForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.regression_head = nn.Linear(config.d_model, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, labels=None):\n",
    "        # Provide default decoder_input_ids if not supplied\n",
    "        if decoder_input_ids is None:\n",
    "            decoder_input_ids = torch.ones((input_ids.shape[0], 1), dtype=torch.long, device=input_ids.device) * self.config.pad_token_id\n",
    "\n",
    "        # Pass inputs through T5 model\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids\n",
    "        )\n",
    "        \n",
    "        # Use the encoder's last hidden state\n",
    "        sequence_output = outputs.encoder_last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Apply the regression head\n",
    "        logits = self.regression_head(sequence_output)\n",
    "\n",
    "        # Convert labels to float\n",
    "        if labels is not None:\n",
    "            labels = labels.float()\n",
    "        \n",
    "        # Calculate the loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.MSELoss()\n",
    "            loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a32d3001-828c-495c-aeba-1e0d0680581a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForRegression were not initialized from the model checkpoint at QizhiPei/biot5-base-mol2text and are newly initialized: ['regression_head.bias', 'regression_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"QizhiPei/biot5-base-mol2text\", model_max_length=512)\n",
    "model = T5ForRegression.from_pretrained('QizhiPei/biot5-base-mol2text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73b3d241-1a25-4363-ae34-18082b4aa064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0cf824aabb450ca166eea3e7882233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3187a2078444d9bb6d232ab77546fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df=pd.read_csv(\"absorption_p1.csv\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[\"scaled_output\"] = scaler.fit_transform(df[[\"output\"]])\n",
    "\n",
    "train, test = train_test_split(df.head(100)[[\"input\", \"scaled_output\"]])\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "eval_dataset = Dataset.from_pandas(test)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"input\"]\n",
    "    model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = examples[\"scaled_output\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f54f6743-1546-48cc-baf3-aa1ef09d5da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kruchkov/biot5/.venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/kruchkov/biot5/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15, training_loss=0.0, metrics={'train_runtime': 19.3728, 'train_samples_per_second': 11.614, 'train_steps_per_second': 0.774, 'total_flos': 155634264345600.0, 'train_loss': 0.0, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define training arguments with reduced learning rate and gradient clipping\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=1e-9,  # Reduced learning rate\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    max_grad_norm=1.0  # Gradient clipping\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f71144a-2dfa-4b53-b12c-1d26d0a23157",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ac770-c0ef-400d-8869-23db8406979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = train.input[0]\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(device)\n",
    "outputs = model(**inputs)\n",
    "predicted_number = outputs[\"logits\"].item()\n",
    "print(f\"Predicted number: {predicted_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dfbb9a-99a2-4203-9b0c-ea5b5f62c579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
